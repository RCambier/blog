{
  
    
        "post0": {
            "title": "Linear regression & Logistic regression",
            "content": "import scipy as sp import numpy as np import pandas as pd from sklearn.metrics import r2_score, precision_score, recall_score from sklearn.linear_model import LinearRegression import sklearn . Some data . df = pd.read_csv(&quot;https://rcambier.github.io/blog/assets/california_housing_train.csv&quot;) df = df[[&#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;, &#39;median_house_value&#39;]] . Linear Regression . scaled_df = (df - df.min()) / (df.max() - df.min()) X = scaled_df[[&#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;]].values y = scaled_df[&#39;median_house_value&#39;].values X_with_intercept = np.hstack((np.ones((len(X), 1)),X)) B = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ (X_with_intercept.T @ y.reshape(-1, 1)) print(&quot;Manual weights: &quot;, B.reshape(-1)) print(&quot;Manual score: &quot;, r2_score(y, (X_with_intercept @ B).reshape(-1))) . Manual weights: [-0.07556544 0.19769139 -1.56087573 1.32234017 -2.57610401 1.59516284 1.43606576] Manual score: 0.5713482748283873 . Let&#39;s compare those results with sklearn linear regression . lr = LinearRegression().fit(X, y) print(&quot;&quot;) print(&quot;Sklearn weights: &quot;, [lr.intercept_] + lr.coef_.tolist() ) print(&quot;Sklearn score: &quot;, r2_score(y, lr.predict(X))) . Sklearn weights: [-0.07556543642855085, 0.19769138728528463, -1.560875734209466, 1.3223401715433833, -2.5761040065353327, 1.5951628411047127, 1.4360657609756604] Sklearn score: 0.5713482748283873 . Logistic Regression . def sigmoid(x): return 1 / (1 + np.exp(-x)) def log_likelihood(y_hat, y_true): return np.sum( y_true * np.log(y_hat) + (1-y_true) * np.log(1-y_hat) ) def gradients(X, y, y_hat): # X --&gt; Input. # y --&gt; true/target value. # y_hat --&gt; hypothesis/predictions. # w --&gt; weights (parameter). # b --&gt; bias (parameter). # m-&gt; number of training examples. m = X.shape[0] # Gradient of loss w.r.t weights. dw = (1/m)*np.dot(X.T, (y_hat - y)) return dw . df[&#39;median_house_value_cat&#39;] = (df[&#39;median_house_value&#39;] &gt; 150_000).astype(int) scaled_df = (df - df.min()) / (df.max() - df.min()) X = scaled_df[[&#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;]].values y = df[&#39;median_house_value_cat&#39;].values X_with_intercept = np.hstack((np.ones((len(X), 1)),X)) B = np.random.normal(0, 0.1 ,(7, 1)) for i in range(20_000): y_hat = sigmoid(X_with_intercept @ B).reshape(-1) if i % 1000 == 0 or i ==0: print(&quot;loss: &quot;, log_likelihood(y_hat, y)) deltas = gradients(X_with_intercept, y, y_hat) B -= 0.3 * deltas.reshape(-1, 1) lr = sklearn.linear_model.LogisticRegression().fit(X, y) . loss: -11871.790341111762 loss: -8989.260203661237 loss: -8361.557282645226 loss: -8089.454906650863 loss: -7948.545641070333 loss: -7867.047626101753 loss: -7815.839761158184 loss: -7781.320051928547 loss: -7756.513094073385 loss: -7737.597861359006 loss: -7722.377200111432 loss: -7709.540423021114 loss: -7698.283076636503 loss: -7688.100187248782 loss: -7678.668978644468 loss: -7669.7800335294505 loss: -7661.295751092525 loss: -7653.124700733372 loss: -7645.205490789696 loss: -7637.496464802648 . print(&quot;Manual weights: &quot;, B.reshape(-1)) print(&quot;Manual score: &quot;, precision_score(y, (sigmoid(X_with_intercept @ B).reshape(-1) &gt; 0.5).astype(int) ), recall_score(y, (sigmoid(X_with_intercept @ B).reshape(-1) &gt; 0.5).astype(int) ), ) print() print(&quot;Sklearn weights: &quot;, lr.intercept_.tolist() + lr.coef_.reshape(-1).tolist()) print(&quot;Sklearn score&quot;, precision_score(y, lr.predict(X)), recall_score(y, lr.predict(X)) ) . Manual weights: [-4.56293299 2.1266067 -3.80393265 3.61846479 -0.96766961 4.42720515 17.74366463] Manual score: 0.8152946468735942 0.8444692945671419 Sklearn weights: [-4.385273936252051, 1.9603353158729624, -10.78106293024599, 6.882196980429938, -2.868679885031378, 7.251350300146187, 17.41000987846787] Sklearn score 0.8186773905272565 0.8536949026185817 . The weights are not exactly the same but the performances are very similar. This is due to the randomness aspect of training through gradient descent. .",
            "url": "https://rcambier.github.io/blog/ai/2021/09/14/linear-regression.html",
            "relUrl": "/ai/2021/09/14/linear-regression.html",
            "date": " ‚Ä¢ Sep 14, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Hypothesis testing",
            "content": "Hypothesis testing . &quot;Hypothesis testing, confidence interval&quot; . comments:true- badges: true | categories: [ai] | publishes: true | . import scipy as sp import numpy as np . A typical statement . A particular brand of tires claims that its deluxe tire averages at least 50,000 miles before it needs to be replaced. From past studies of this tire, the standard deviation is known to be 8,000. A survey of owners of that tire design is conducted. From the 28 tires surveyed, the mean lifespan was 46,500 miles with a standard deviation of 9,800 miles. Using ùõº=0.05 , is the data highly inconsistent with the claim? . claim_pop_mean = 50_000 pop_std = 8000 # What we know of the sample n = 28 sample_mean = 46_500 sample_std = 9800 # The chances of Type 1 error we are ready to accept alpha = 0.05 . The question can be formulated as: . &quot;Compared to the mean of that population (50_000), how crazy is the sample mean (46_500) ? With an alpha of 0.05&quot; | . which becomes . &quot;Using the sample deviation of the mean of that population, how far is the sample mean ? With an alpha of 0.05&quot; | . which becomes . &quot;Is the sample mean further away than 1.64 times the standard error of that population ?&quot; | . # H0 =&gt; pop_mean &gt;= 50_000 # H1 =&gt; pop_mean &lt; 50_000 population_standard_error = 8000 / np.sqrt(28) # &quot;If you grab a random sample mean, how is it going to variate&quot; how_far_we_are_from_pop_mean = (46_500 - 50_000) / population_standard_error # How far is this specific sample mean from the population mean. . There are different ways to reject the null hypothesis. . We can look at wether we are smaller or not than 0.05. . how_far_we_are_in_z = sp.stats.norm.cdf(how_far_we_are_from_pop_mean) how_far_we_are_in_z . 0.010305579572800304 . In this case we are at 0.01, which means that in the distribution of sample means, we are so extreme that there is no way that the sample mean we observed actually came from the sample mean distribution that we built looking at the population. . Another way is to look at how far we go on the axis, not in term of percentage (like 0,05 being 5%) but in term of distance from the population mean. This would look like the following . how_far_we_are_from_pop_mean . -2.315032397181517 . To know if this is a value too extreme or not, we can compare it to how far 0.05 is on the same axis: . - sp.stats.norm.ppf(0.95) . -1.6448536269514722 . When you don&#39;t have the population standard deviation . Realistically however, you often don&#39;t have the population standard deviation. In this case, you need to estimate it from the sample. . Doing that is less accurate. In order to compensate a bit, we need to model the &quot;spread of sample means&quot; a bit differently. . Since normally we allow the sample mean to only go &quot;so far&quot; from the population mean. We will force it to be &quot;even a bit further&quot;. The way we do this is by using a &quot;heavy tail&quot; distribution for the sample mean. That way, the 0.05 mark will be further to the right or to the left, and we are forced to be a little bit more sure of ourselves before saying anything. . Let&#39;s use the sample problem as above, but pretend that we don&#39;t know that the population has a standard deviation of 8000. We are forced to use the 9800 that we discovered experimentally. . claim_pop_mean = 50_000 # pop_std = 8000 # we don&#39;t know this anymore # What we know of the sample n = 28 sample_mean = 46_500 sample_std = 9800 # The chances of Type 1 error we are ready to accept alpha = 0.05 # 1. How far is the sample_mean from the pop_mean ? # H0 =&gt; pop_mean &gt;= 50_000 # H1 =&gt; pop_mean &lt; 50_000 population_standard_error = 9800 / np.sqrt(28) # &quot;If you grab a random sample mean, how is it going to variate&quot; how_far_we_are_from_pop_mean = (46_500 - 50_000) / population_standard_error # How far is this specific sample mean from the population mean. how_far_we_are_in_z = sp.stats.t.cdf(how_far_we_are_from_pop_mean, df=n-1) how_far_we_are_in_z . 0.014225814767264972 . We still reject the null hypothesis. But notice how much less confident we are ! Even if the standard deviation we sample was exactly 8000 (like the population one), we would still be less confident than if we received the standard deviation through a trustful source. . This is the whole point of this T student distribution ! . Confidence interval . Confidence interval are only in the point of view of the sample we just took. . From that sample, let&#39;s just add a standard error on each side and see how far this goes. . how_much_we_allow_on_unit_normal_distrib = sp.stats.norm.ppf(0.95) sample_mean_standard_error = 9800 / np.sqrt(n) how_much_we_allow_in_problem_domain = how_much_we_allow_on_unit_normal_distrib * sample_mean_standard_error how_much_we_allow_in_problem_domain [46_500 - how_much_we_allow_in_problem_domain, 46_500 + how_much_we_allow_in_problem_domain] . 3046.311548011343 .",
            "url": "https://rcambier.github.io/blog/2021/09/13/hypothesis-testing.html",
            "relUrl": "/2021/09/13/hypothesis-testing.html",
            "date": " ‚Ä¢ Sep 13, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Three‚Äòs a Crowd",
            "content": "From: http://www.twinbear.com/riddles.html . (Clayton Lewis) After solving the riddle of the three wise folks, three scoundrels claim to be the smartest in the country. So you decide to give them a challenge. Suspecting that the thing they care about most is money, you give them $100 and tell them they are to divide this money observing the following rule: they are to discuss offers and counter-offers from each other and then take a vote. Majority vote wins. Sounds easy enough‚Ä¶ now the question is, assuming each person is motivated to take the largest amount possible, what will the outcome be? . Note: careful‚Ä¶ if the answer were that they split it 50% / 50% / 0%, or 1/3 / 1/3 / 1/3, it wouldn‚Äòt be a riddle! . Note: careful‚Ä¶ 96.6523544 % of people who send answers to this have not thought about it for even 1 minute. I guarantee you won‚Äòt solve it in a minute. (96.6523544% of the time this guarantee is correct.) . Hover to show the answer. . I can not solve this one so far. If you can please share your answer! .",
            "url": "https://rcambier.github.io/blog/riddle/2021/01/05/scoundrels.html",
            "relUrl": "/riddle/2021/01/05/scoundrels.html",
            "date": " ‚Ä¢ Jan 5, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "The newcomb poison",
            "content": "You are put into a room with a suitcase and a flask of poison. . The suitcase has been filled either with 1 million dollar or with worthless magazines. . The poison is bad enough that you would never drink it for free, but it will not kill you. You would definitely drink it for 1 million dollar. It is going to give you a few days of headaches and feeling very ill. . Wether the suitace contains money or magazines depends on a prediction that was made by a machine. . If the machine predicts that you will drink the poison, the suitcase will contain the money | If the machine predicts that you will not drink the poison, the suitcase will be full of magazines. | . From more than 10 000 past experiments, we know that the machine has 95% accuracy on both cases. . You are now in the room. The prediction has been made on you and the suitcase has been filled accordingly. . Do you drink the poison and open the suitcase ? Or do you open the suitcase right away ? . Note: This is taken from Mr Phi video serie where he discusses the NewComb Paradox . Hover to show the answer. . My understanding at this time (early 2021) is the following: As a human being, you can not make a plan and stick to it. This would mean inhibiting your rationality. This is something that, unfortunately, we can not do. . So even if you 100% plan to drink the poison, you will doubt that decision once you are in the room. . I think I am not able to stick to a plan of drinking the poison. Therefore, I think I would not drink the poison and go straight for the suitcase. . This is concluding that there is no way for me to win the money. . Someone that could inhibit his rationality and stick to a plan could win the money. Someone not thinking about it too much and going for the poison could win the money. But as soon as you think too much about it, you understand that there is no way you actually drink the poison once you are in the room. . To help explain it: . The extreme case of a transparent suitcase. In that case, everyone will have to answer that they don‚Äôt drink the poison. | The extreme case of someone that can turn off his rationality. That person will be able to stick to his plan of drinking and actually win the money. But this is not possible for a normal human being. Look for the ‚Äúfatality of rationality‚Äù for this concept. | . | .",
            "url": "https://rcambier.github.io/blog/riddle/2021/01/05/newcomb.html",
            "relUrl": "/riddle/2021/01/05/newcomb.html",
            "date": " ‚Ä¢ Jan 5, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "The blind pill",
            "content": "You are blind. You have 2 blue pills and 2 red pills in a box. . You have to take 2 pills today and 2 tomorrow. Each day, exactly one blue and one red. If you don‚Äôt, you will die. If you take more, you will die. . How do you do it ? . Hover to show the answer. . One by one, you take the pill, break it in half, eat a half and keep the other half for tomorrow. This way, you will eat exactly 1 blue and 1 red pill per day. .",
            "url": "https://rcambier.github.io/blog/riddle/2020/07/29/pills.html",
            "relUrl": "/riddle/2020/07/29/pills.html",
            "date": " ‚Ä¢ Jul 29, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "The rope triangle",
            "content": "You cut a rope in a random place. Then again in another random place. . You are left with 3 pieces. . What are the chances that you can form a triangle with those 3 pieces ? . Hover to show the answer. . 1/4. .",
            "url": "https://rcambier.github.io/blog/riddle/2020/04/19/rope-triangle.html",
            "relUrl": "/riddle/2020/04/19/rope-triangle.html",
            "date": " ‚Ä¢ Apr 19, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Reversed number",
            "content": "Can you find a 4 digits number that gets reversed when multiplied by 4? . For example: if 4*1234 == 4321, 1234 would be an answer. Unfortunately, that is not the case. All digits need to be different. . Hover to show the answer. . 2178. .",
            "url": "https://rcambier.github.io/blog/riddle/2020/04/19/reverse-number.html",
            "relUrl": "/riddle/2020/04/19/reverse-number.html",
            "date": " ‚Ä¢ Apr 19, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.‚Ü© . 2. This is the other footnote. You can even have a link!‚Ü© .",
            "url": "https://rcambier.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Ai from scratch",
            "content": "import scipy as sp import numpy as np import pandas as pd import seaborn as sns import plotly.graph_objects as go import sklearn from sklearn.linear_model import LinearRegression import pymc3 from sklearn.metrics import r2_score, precision_score, recall_score, confusion_matrix . Hypothesis testing . A particular brand of tires claims that its deluxe tire averages at least 50,000 miles before it needs to be replaced. From past studies of this tire, the standard deviation is known to be 8,000. A survey of owners of that tire design is conducted. From the 28 tires surveyed, the mean lifespan was 46,500 miles with a standard deviation of 9,800 miles. Using ùõº=0.05 , is the data highly inconsistent with the claim? . claim_pop_mean = 50_000 pop_std = 8000 n = 28 sample_mean = 46_500 sample_std = 9800 alpha = 0.05 . # H0 =&gt; pop_mean = 50_000 # H1 =&gt; pop_mean &gt; 50_000 or pop_mean &lt; 50_000 print(&quot;If we know the pop std&quot;) how_far_on_the_unit_normal_of_sample_means = (46_500 - 50_000) / (8000/np.sqrt(28) ) print(how_far_on_the_unit_normal_of_sample_means, &quot; how_far_on_the_unit_normal_of_sample_means&quot;) how_far_we_allow = - sp.stats.norm.ppf(0.95) print(how_far_we_allow, &quot; how_far_we_allow&quot;) how_far_we_are_percent = sp.stats.norm.cdf(how_far_on_the_unit_normal_of_sample_means) print(how_far_we_are_percent, &quot; how_far_we_are_percent&quot;) print(&quot; n&quot;) print(&quot;If we don&#39;t know the pop std&quot;) how_far_on_the_unit_normal_of_sample_means = (46_500 - 50_000) / (9800/np.sqrt(28) ) print(how_far_on_the_unit_normal_of_sample_means, &quot; how_far_on_the_unit_normal_of_sample_means&quot;) how_far_we_allow = - sp.stats.t.ppf(0.95, df=28-1) print(how_far_we_allow, &quot; how_far_we_allow&quot;) how_far_we_are_percent = sp.stats.t.cdf(how_far_on_the_unit_normal_of_sample_means, df=28-1) print(how_far_we_are_percent, &quot; how_far_we_are_percent&quot;) . If we know the pop std -2.315032397181517 how_far_on_the_unit_normal_of_sample_means -1.6448536269514722 how_far_we_allow 0.010305579572800304 how_far_we_are_percent If we don&#39;t know the pop std -1.8898223650461363 how_far_on_the_unit_normal_of_sample_means -1.7032884457221265 how_far_we_allow 0.034781617022391444 how_far_we_are_percent . Confidence interval . how_much_we_allow_on_unit_normal_distrib = sp.stats.norm.ppf(0.95) how_much_we_allow_in_problem_domain = how_much_we_allow_on_unit_normal_distrib * (9800 / np.sqrt(n)) how_much_we_allow_in_problem_domain . 3046.311548011343 . [46_500 - how_much_we_allow_in_problem_domain, 46_500 + how_much_we_allow_in_problem_domain] . [43453.68845198866, 49546.31154801134] . Bayesian inference . fake_observed = sp.stats.norm(46, 9).rvs(size=28) fake_observed.mean(), fake_observed.std() . (45.48803701443949, 8.076116332902322) . from scipy.stats import norm, binom import matplotlib.pyplot as plt possible_probabilities_mean = np.linspace(0,100,100) prior_mean = norm.pdf(possible_probabilities_mean, loc=60, scale=20) prior_std = 9 plt.plot(prior_mean, label=&quot;prior&quot;) likelihood_mean = norm.pdf(fake_observed.mean(), loc=possible_probabilities_mean, scale=9) plt.plot(likelihood_mean, label=&quot;likelihood&quot;) posterior_unnormalized = prior_mean * likelihood_mean posterior = posterior_unnormalized / posterior_unnormalized.sum() plt.plot(posterior, label=&quot;posterior&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f89fea053d0&gt; . with pymc3.Model() as model: u_prior = pymc3.distributions.Uniform(&quot;u_prior&quot;, 0, 100) sigma_prior = pymc3.distributions.Uniform(&quot;sigma_prior&quot;, 0, 20) likelihood = pymc3.distributions.Normal(&quot;likelihood&quot;, mu=u_prior, sigma=sigma_prior, observed=[fake_observed]) trace = pymc3.sample() pymc3.traceplot(trace) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning. Auto-assigning NUTS sampler... Initializing NUTS using jitter+adapt_diag... Sequential sampling (2 chains in 1 job) NUTS: [sigma_prior, u_prior] . . 100.00% [2000/2000 00:02&lt;00:00 Sampling chain 0, 0 divergences] . 100.00% [2000/2000 00:01&lt;00:00 Sampling chain 1, 0 divergences] Sampling 2 chains for 1_000 tune and 1_000 draw iterations (2_000 + 2_000 draws total) took 4 seconds. /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: The function `traceplot` from PyMC3 is just an alias for `plot_trace` from ArviZ. Please switch to `pymc3.plot_trace` or `arviz.plot_trace`. /usr/local/lib/python3.7/dist-packages/arviz/data/io_pymc3.py:100: FutureWarning: Using `from_pymc3` without the model will be deprecated in a future release. Not using the model will return less accurate and less useful results. Make sure you use the model argument or call from_pymc3 within a model context. . array([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f89f3d018d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f89f326ded0&gt;], [&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f89f32aa890&gt;, &lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f89f32aeb90&gt;]], dtype=object) . Linear Regression . df = pd.read_csv(&quot;sample_data/california_housing_train.csv&quot;) df = df[[&#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;, &#39;median_house_value&#39;]] scaled_df = (df - df.min()) / (df.max() - df.min()) X = scaled_df[[&#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;]].values y = scaled_df[&#39;median_house_value&#39;].values X_with_intercept = np.hstack((np.ones((len(X), 1)),X)) B = np.linalg.inv(X_with_intercept.T @ X_with_intercept) @ (X_with_intercept.T @ y.reshape(-1, 1)) lr = LinearRegression().fit(X, y) print(&quot;Manual: &quot;, B) print(&quot;Manual score: &quot;, r2_score(y, (X_with_intercept @ B).reshape(-1))) print(&quot;&quot;) print(&quot;Sklearn: &quot;, lr.coef_, lr.intercept_) print(&quot;Sklearn score: &quot;, r2_score(y, lr.predict(X))) . Manual: [[-0.07556544] [ 0.19769139] [-1.56087573] [ 1.32234017] [-2.57610401] [ 1.59516284] [ 1.43606576]] Manual score: 0.5713482748283873 Sklearn: [ 0.19769139 -1.56087573 1.32234017 -2.57610401 1.59516284 1.43606576] -0.07556543642855107 Sklearn score: 0.5713482748283873 . Logistic Regression . df[&#39;median_house_value&#39;].hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8a03efec10&gt; . def sigmoid(x): return 1 / (1 + np.exp(-x)) def log_likelihood(y_hat, y_true): return np.sum( y_true * np.log(y_hat) + (1-y_true) * np.log(1-y_hat) ) def gradients(X, y, y_hat): # X --&gt; Input. # y --&gt; true/target value. # y_hat --&gt; hypothesis/predictions. # w --&gt; weights (parameter). # b --&gt; bias (parameter). # m-&gt; number of training examples. m = X.shape[0] # Gradient of loss w.r.t weights. dw = (1/m)*np.dot(X.T, (y_hat - y)) return dw . df = pd.read_csv(&quot;sample_data/california_housing_train.csv&quot;) df = df[[&#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;, &#39;median_house_value&#39;]] df[&#39;median_house_value_cat&#39;] = (df[&#39;median_house_value&#39;] &gt; 150_000).astype(int) scaled_df = (df - df.min()) / (df.max() - df.min()) X = scaled_df[[&#39;housing_median_age&#39;, &#39;total_rooms&#39;, &#39;total_bedrooms&#39;, &#39;population&#39;, &#39;households&#39;, &#39;median_income&#39;]].values y = df[&#39;median_house_value_cat&#39;].values X_with_intercept = np.hstack((np.ones((len(X), 1)),X)) B = np.random.normal(0, 0.1 ,(7, 1)) for i in range(20_000): y_hat = sigmoid(X_with_intercept @ B).reshape(-1) if i % 1000 == 0 or i ==0: print(&quot;loss: &quot;, log_likelihood(y_hat, y)) deltas = gradients(X_with_intercept, y, y_hat) B -= 0.3 * deltas.reshape(-1, 1) lr = sklearn.linear_model.LogisticRegression().fit(X, y) . loss: -11921.545904003113 loss: -8967.128160745051 loss: -8351.305989089486 loss: -8083.278801524377 loss: -7944.149245248092 loss: -7863.547173838429 loss: -7812.837065054392 loss: -7778.616079912635 loss: -7754.000783091298 loss: -7735.216404729813 loss: -7720.090594375023 loss: -7707.32663707406 loss: -7696.128260982457 loss: -7685.995432558281 loss: -7676.608432478169 loss: -7667.759772286712 loss: -7659.313087143786 loss: -7651.177747927787 loss: -7643.292887269918 loss: -7635.617193645834 . print(&quot;Manual: &quot;, B) print(&quot;Manual score: &quot;, precision_score(y, (sigmoid(X_with_intercept @ B).reshape(-1) &gt; 0.5).astype(int) ), recall_score(y, (sigmoid(X_with_intercept @ B).reshape(-1) &gt; 0.5).astype(int) ), ) print() print(&quot;Sklearn: &quot;, lr.coef_, lr.intercept_) print(&quot;Sklearn score&quot;, precision_score(y, lr.predict(X)), recall_score(y, lr.predict(X)) ) . Manual: [[-4.56223472] [ 2.12654314] [-3.80572231] [ 3.60375804] [-1.19804952] [ 4.550525 ] [17.74264368]] Manual score: 0.8152614055610546 0.8442829186469109 Sklearn: [[ 1.96033532 -10.78106293 6.88219698 -2.86867989 7.2513503 17.41000988]] [-4.38527394] Sklearn score 0.8186773905272565 0.8536949026185817 . Confusion Matrix . sns.heatmap(confusion_matrix(y, (sigmoid(X_with_intercept @ B).reshape(-1) &gt; 0.5).astype(int)), annot=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd80a75cb10&gt; . Decision tree . from sklearn.datasets import load_wine, load_breast_cancer import pandas as pd import numpy as np from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor from sklearn.model_selection import cross_val_score from sklearn.metrics import f1_score . load_breast_cancer()[&#39;feature_names&#39;][22] . &#39;worst perimeter&#39; . X, y = load_breast_cancer(return_X_y=True) . f1_score(y, np.round(y.mean().repeat(len(y)))) . 0.7710583153347732 . X = pd.DataFrame(X) smallest_gini_score = float(&quot;inf&quot;) threshold_saved = None # Pick a row and column to use as threshold for col in X.columns: for row in X.index: threshold = X.iloc[row, col] left_leaf_idx = X.loc[:, col] &gt;= threshold right_leaf_idx = X.loc[:, col] &lt; threshold if sum(left_leaf_idx) &gt; 0 and sum(right_leaf_idx) &gt; 0: # Compute the gini score with that threshold, and save it if it&#39;s the smallest so far gini_score_left = (1 - y[left_leaf_idx].mean()**2 - (1-y[left_leaf_idx].mean())**2) gini_score_right = (1 - y[right_leaf_idx].mean()**2 - (1-y[right_leaf_idx].mean())**2) gini_score = (sum(left_leaf_idx) * gini_score_left + sum(right_leaf_idx) * gini_score_right) / len(X) if gini_score &lt; smallest_gini_score: smallest_gini_score = gini_score threshold_saved = (col, row, threshold) # We now use that threshold to split print(threshold_saved) . (20, 31, 16.82) . left_leaf_idx = X.loc[:, 20] &gt;= 16.82 right_leaf_idx = X.loc[:, 20] &lt; 16.82 y_predict = pd.Series(y.copy()) y_predict[left_leaf_idx] = y[left_leaf_idx].mean() y_predict[right_leaf_idx] = y[right_leaf_idx].mean() print(&quot;Leaf sizes: &quot;, len(left_leaf_idx), len(right_leaf_idx)) print(&quot;Leaf values: &quot;, y[left_leaf_idx].mean(), y[right_leaf_idx].mean()) print(&quot;F1 score: &quot;, f1_score(y, np.round(y_predict))) . Leaf sizes: 569 569 Leaf values: 0.05789473684210526 0.9129287598944591 F1 score: 0.9402173913043479 . from sklearn import tree t = DecisionTreeClassifier(max_depth=1, criterion=&#39;gini&#39;) t.fit(X, y) print(tree.export_text(t)) . | feature_20 &lt;= 16.80 | | class: 1 | feature_20 &gt; 16.80 | | class: 0 . Random forest TODO . from sklearn.datasets import load_wine, load_breast_cancer import pandas as pd import numpy as np from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor from sklearn.model_selection import cross_val_score from sklearn.metrics import f1_score . X, y = load_breast_cancer(return_X_y=True) . for i in range(30): # 1. Use boosting to sample the data # 2. Pick a random subset of the features tree = DecisionTreeRegressor() tree.fit(X, y) . Boosting trees . from sklearn.datasets import load_wine, load_breast_cancer import pandas as pd import numpy as np from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor from sklearn.model_selection import cross_val_score from sklearn.metrics import f1_score . raw = load_breast_cancer(return_X_y=True) X = pd.DataFrame(raw[0]) y = pd.DataFrame(raw[1]) . initial_prediction_proba = y.mean() initial_prediction_classes = round(initial_prediction_proba) initial_prediction_logodds = np.log(initial_prediction_proba / (1-initial_prediction_proba)) proba_residuals = (y - initial_prediction_proba).values.reshape(-1) print(&quot;Score with mean: &quot;, f1_score(y, [1]*len(y))) trees = [] for i in range(50): tree = DecisionTreeRegressor(max_depth=1) tree.fit(X, proba_residuals) trees.append(tree) proba_residuals = proba_residuals - tree.predict(X).reshape(-1) predictions = np.array(y.mean()) for tree in trees: predictions = tree.predict(X).reshape(-1) + predictions print(&quot;Final score: &quot;, f1_score(y, predictions.round())) . Score with mean: 0.7710583153347732 Final score: 0.9902370990237099 . Gradient boosting tree . Distributions TODO . Normal distribution . Poisson distribution . from scipy.stats import poisson . PCA . The principal components are the eigenvectors+eigenvalues of the Covariance matrix of our data. . This is because we are looking for the &quot;Direction of stretching and how much streching happens&quot; regarding the variance of our data. . from sklearn.datasets import load_digits import seaborn as sns from sklearn.decomposition import PCA import pandas as pd import numpy as np digits = pd.DataFrame(load_digits()[&#39;data&#39;]) classes = load_digits(return_X_y=True)[1] . low_dim_digits = PCA(n_components=2).fit_transform(digits) sns.scatterplot(x=low_dim_digits[:,0], y=low_dim_digits[:,1], hue=classes) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f80ce1fe210&gt; . digits_normed = digits - digits.mean() # compute the covariance matrix cov_matrix = digits_normed.T @ digits_normed / len(digits_normed) # digits_normed.cov() eigen_values, eigen_vectors = np.linalg.eig(cov_matrix) eigen_values, eigen_vectors # Sort eigen values end eigen vectors sorted_index = np.argsort(eigen_values)[::-1] sorted_eigenvalue = eigen_values[sorted_index] sorted_eigenvectors = eigen_vectors[:,sorted_index] # Select the 2 best eigenvector_subset = sorted_eigenvectors[:, 0:2] X_reduced = np.dot(eigenvector_subset.transpose(), digits_normed.transpose()).transpose() sns.scatterplot(x=X_reduced[:,0], y=X_reduced[:,1], hue=classes) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f80ce362a10&gt; . # Ce texte est au format code . Matrix factorization . SVD . import pandas as pd import numpy as np raw = pd.read_csv(&quot;https://raw.githubusercontent.com/smanihwr/ml-latest-small/master/ratings.csv&quot;) user_item_interactions = raw.pivot(values=&quot;rating&quot;, columns=&quot;movieId&quot;, index=&quot;userId&quot;) user_item_interactions = user_item_interactions.fillna(0) . from scipy.linalg import eig . import numpy as np A = np.array([ [5,5,0,1], [5,5,0,0], [0,1,5,5], [0,0,5,5], [0,0,3,5] ]) # A = np.array([ # [3, 2, 2], # [2, 3, -2], # ]) U_eigen_values, U_unordered = np.linalg.eig(A @ A.T) V_eigen_values, V_unordered = np.linalg.eig(A.T @ A) idx_U = np.argsort(U_eigen_values)[::-1] idx_V = np.argsort(V_eigen_values)[::-1] D = np.sqrt(np.around(V_eigen_values[idx_V], decimals=10)) U = U_unordered[:,idx_U] # Using the order of U_eigen_values to reorder U U = U * [[1,1,-1,-1,-1]] # Each eigenvector can be in 2 directions. Pick the correct one. Very manual. I actually based it on the result of np.linalg.svd. Not sure how you should actually be doing this. V = (V_unordered[:,idx_V] * [[-1, -1, 1, -1]]) # Using the order of V_eigen_values to reorder V. # Each eigenvector can be in 2 directions. Pick the correct one. Very manual. . np.around(np.matrix(U) @ np.vstack((np.diag(D), np.zeros((len(V))))) @ np.matrix(V.T), decimals=1) . array([[ 5., 5., -0., 1.], [ 5., 5., 0., -0.], [ 0., 1., 5., 5.], [-0., 0., 5., 5.], [ 0., -0., 3., 5.]]) . U_, D_, Vt_ = np.linalg.svd(A) np.around(np.matrix(U_) @ np.vstack((np.diag(D_), np.zeros((len(V_))))) @ np.matrix(Vt_), decimals=1) . array([[ 5., 5., 0., 1.], [ 5., 5., -0., 0.], [-0., 1., 5., 5.], [-0., -0., 5., 5.], [-0., -0., 3., 5.]]) . Truncated SVD . Truncate the SVD to 2 components by only keeping the two bigest eigenvalues . np.matrix(U[:, :2]) . matrix([[-0.23093819, -0.66810948], [-0.16863574, -0.68636674], [-0.59892473, 0.13274366], [-0.57986295, 0.20070102], [-0.47252267, 0.15693514]]) . np.around(np.matrix(U[:, :2]) @ np.diag(D[:2]) @ np.matrix(V[:,:2].T), decimals=1) . array([[ 5. , 5. , 0.3, 0.8], [ 5. , 5. , -0.2, 0.2], [ 0.3, 0.7, 4.7, 5.3], [-0.2, 0.2, 4.7, 5.3], [-0.1, 0.2, 3.8, 4.3]]) . Using gradient descent TODO . import numpy as np A = np.array([ [5,5,0,1], [5,5,0,0], [0,1,5,5], [0,0,5,5], [0,0,3,5] ]) U = np.random.rand(5, 3) * 0.01 D = np.random.rand(4, 3) * 0.01 . error = ((A - U @ D.T)**2).mean() deltas = A - U @ D.T . 11.799389714228234 . https://medium.com/analytics-vidhya/matrix-factorization-made-easy-recommender-systems-7e4f50504477 . Neural network TODO . import numpy as np X = np.array([[1,0] , [0,0], [0,1], [1,1]]) y = np.array([[0,1] , [1,0], [0,1], [1,0]]) weights_input_hidden = np.random.rand(2, 4) * 0.001 weights_hidden_output = np.random.rand(4, 2) * 0.001 def relu(data): return np.maximum(data, 0) def relu_grad(data): return (data &gt; 0 )* 1 def softmax(x): e_x = np.exp(x) - x.sum(axis=1).reshape(-1, 1) return e_x / e_x.sum(axis=1).reshape(-1, 1) def softmax_grad(softmax): s = softmax.reshape(-1,1) return np.diagflat(s) - np.dot(s, s.T) predictions = softmax(relu(X @ weights_input_hidden) @ weights_hidden_output) errors = . array([[0.50000012, 0.49999988], [0.5 , 0.5 ], [0.49999997, 0.50000003], [0.50000009, 0.49999991]]) . softmax_grad(predictions) . array([[ 0.25 , -0.25 , -0.24999999, -0.24999999, -0.24999996, -0.25000002, -0.24999994, -0.25000003], [-0.25 , 0.25 , -0.25000001, -0.25000001, -0.24999998, -0.25000004, -0.24999997, -0.25000006], [-0.24999999, -0.25000001, 0.25 , -0.25 , -0.24999997, -0.25000003, -0.24999996, -0.25000004], [-0.24999999, -0.25000001, -0.25 , 0.25 , -0.24999997, -0.25000003, -0.24999996, -0.25000004], [-0.24999996, -0.24999998, -0.24999997, -0.24999997, 0.25 , -0.25 , -0.24999992, -0.25000001], [-0.25000002, -0.25000004, -0.25000003, -0.25000003, -0.25 , 0.25 , -0.24999999, -0.25000008], [-0.24999994, -0.24999997, -0.24999996, -0.24999996, -0.24999992, -0.24999999, 0.25 , -0.25 ], [-0.25000003, -0.25000006, -0.25000004, -0.25000004, -0.25000001, -0.25000008, -0.25 , 0.25 ]]) . Loss functions TODO .",
            "url": "https://rcambier.github.io/blog/ai/2020/01/28/ai_basics.html",
            "relUrl": "/ai/2020/01/28/ai_basics.html",
            "date": " ‚Ä¢ Jan 28, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a ‚Äúlevel 1 heading‚Äù in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here‚Äôs a footnote 1. Here‚Äôs a horizontal rule: . . Lists . Here‚Äôs a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes ‚Ä¶and‚Ä¶ . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote.¬†&#8617; . |",
            "url": "https://rcambier.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " ‚Ä¢ Jan 14, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Surprise birthday",
            "content": "Albert and Bernard just become friends with Cheryl, and they want to know when her birthday is. Cheryl gives them a list of 10 possible dates. . May 15 | May 16 | May 19 | June 17 | June 18 | July 14 | July 16 | August 14 | August 15 | August 17 | . Chely then tells Albert and Bernard separately the month and the day of her birthday respectively. . Albert: I don‚Äôt know when Cheryl‚Äôs birthday is, but I know that Bernard does not know too. . Bernard: At first I don‚Äôt know when Cheryl‚Äôs birthday is, but I know now. . Albert: Then I also know when Cheryl‚Äôs birthday is. . So when is Cheryl‚Äôs birthday ? . Hover to show the answer. . It is the July 16 . Albert (A) knows the month. | Bernard (B) knows the day. | If A knows the month and knows that B doesn‚Äôt know, it means that it is a month that only contains day that you can also find in other proposed months. Otherwise A wouldn‚Äôt be so sure that B doens‚Äôt know. So it cannot be May because if it was May 19, B would know. It cannot be June, because if it was June 18, B would know. It can be July or August. | B says he now knows when the birthday is. B just deduced, like us, that it could be July or August because of what A just said. If he now knows when it is, it cannot be the 14. So it is the July 16, August 15 or August 17. | A then says that he also now knows. So it has to be the July 16, since that is the only month where A has enough information to know the exact birthdate. If it was August, A would still need more information to differentiate between August 15 and August 17. | .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/26/surprise-birthday.html",
            "relUrl": "/riddle/2018/12/26/surprise-birthday.html",
            "date": " ‚Ä¢ Dec 26, 2018"
        }
        
    
  
    
        ,"post11": {
            "title": "Simple Maths",
            "content": "How do you make 26 by using 5, 5, 5 and 1. You have to use each exactly once. You can use the basic math operations (+, -, *, /, (, ) ). . Hover to show the answer. . (5 + (1/5))*5 = 26 .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/26/simple-maths.html",
            "relUrl": "/riddle/2018/12/26/simple-maths.html",
            "date": " ‚Ä¢ Dec 26, 2018"
        }
        
    
  
    
        ,"post12": {
            "title": "Potatoes",
            "content": "A farmer has 100kg of potatoes. At the start, they are composed of 99% of water (the water is 99% of the total weight) and 1% of dry matter (the dry matter is 1% of the total weight). Later, during storage and because of evaporation, the water percentage drops to 98%. What is the total weight of the potatoes then? . Hover to show the answer. . 50kg. . At the start, there are 99kg of water and 1kg of dry matter. Later, the quantity of dry matter didn‚Äôt change, but it is now representing 2% of the total mass. If 1kg is 2%, then 100% is 50kg. .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/26/potatoes.html",
            "relUrl": "/riddle/2018/12/26/potatoes.html",
            "date": " ‚Ä¢ Dec 26, 2018"
        }
        
    
  
    
        ,"post13": {
            "title": "4 Pieces",
            "content": "There are 4 coins on a quadrant. They are not visible to you. . . If the 4 coins are heads or the four coins are tails, the center light will light up. You have 5 turns to make the center light light up. Each turn, you can choose 2 coins that you reveal (you see if they are heads or tails) and you can flip any of them if you want to. You can flip one, both, or none. After that, they are hidden back from you. After each turn, we spin the quadrant so fast that you can not keep track of wich two coins you interacted with last turn. If the coins are randomly initialized, what strategy allows you to be sure to turn the light on ? . Hover to show the answer. . By following this strategy, you will be sure to turn on the light. .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/26/4-pieces.html",
            "relUrl": "/riddle/2018/12/26/4-pieces.html",
            "date": " ‚Ä¢ Dec 26, 2018"
        }
        
    
  
    
        ,"post14": {
            "title": "Running in circle",
            "content": "If I can make it around a running track once by going 10 km/h, what speed should I be running a second lap to have an average speed of 20 km/h for the two laps ? . Hover to show the answer. . This is impossible, I need to be infinitely fast. . Imagine a 10km track. It will take me 1 hour to make a lap at 10km/h. With a second lap, it becomes 20km. To have a 20km/h average speed, I need to do those 2 laps in 1 hour. . But I already spent 1 hour, so I should be infinitely fast for the second lap, which is impossible. .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/25/running-in-circle.html",
            "relUrl": "/riddle/2018/12/25/running-in-circle.html",
            "date": " ‚Ä¢ Dec 25, 2018"
        }
        
    
  
    
        ,"post15": {
            "title": "Prisoner's switch",
            "content": "The warden meets with 23 new prisoners when they arrive. He tells them, ‚ÄúYou may meet today and plan a strategy. But after today, you will be in isolated cells and will have no communication with one another. . ‚ÄúIn the prison there is a switch room which contains two light switches labeled A and B, each of which can be in either the ‚Äòon‚Äô or the ‚Äòoff‚Äô position. BOTH SWITCHES ARE IN THEIR OFF POSITIONS NOW.* The switches are not connected to anything. . ‚ÄúAfter today, from time to time whenever I feel so inclined, I will select one prisoner at random and escort him to the switch room. This prisoner will select one of the two switches and reverse its position. He must move one, but only one of the switches. He can‚Äôt move both but he can‚Äôt move none either. Then he‚Äôll be led back to his cell.‚Äù . ‚ÄúNo one else will enter the switch room until I lead the next prisoner there, and he‚Äôll be instructed to do the same thing. I‚Äôm going to choose prisoners at random. I may choose the same guy three times in a row, or I may jump around and come back.‚Äù . ‚ÄúBut, given enough time, everyone will eventually visit the switch room as many times as everyone else. At any time any one of you may declare to me, ‚ÄòWe have all visited the switch room.‚Äô . ‚ÄúIf it is true, then you will all be set free. If it is false, and somebody has not yet visited the switch room, you will be fed to the alligators.‚Äù . *note - the only difference from Scenario B, the original position of the 2 switches are known. . Hover to show the answer. . The strategy is the following: . They choose one prisoner that will be the counter. Let‚Äôs name the two switches A and B. The counter is the only prisoner allowed to turn switch A off. When the other prisoner enter the room, if they have never done it, and if the switch A is off, they put it to ON. If they already put the A switch to ON or if they enter en the A switch is ON, they simply switch the B switch, in whatever position. . The counter will be able to count the number of prisoner that have entered the room by counting the number of times he has to put the A switch to OFF. .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/25/prisoners-switch.html",
            "relUrl": "/riddle/2018/12/25/prisoners-switch.html",
            "date": " ‚Ä¢ Dec 25, 2018"
        }
        
    
  
    
        ,"post16": {
            "title": "King's poison",
            "content": "In a far away land, it was known that if you drank poison, the only way to save yourself is to drink a stronger poison in the next 12 hours, which neutralizes the weaker poison. . The king that ruled the land wanted to make sure that he possessed the strongest poison in the kingdom, in order to ensure his survival, in any situation. So the king called the kingdom‚Äôs pharmacist and the kingdom‚Äôs treasurer, he gave each a week to make the strongest poison. Then, each would drink the other one‚Äôs poison, then his own, and the one that will survive, will be the one that had the stronger poison. . The pharmacist went straight to work, but the treasurer knew he had no chance, for the pharmacist was much more experienced in this field, so instead, he made up a sneaky plan to survive and make sure the pharmacist dies. . On the last day the pharmacist suddenly realized that the treasurer would know he had no chance, so he must have a plan. After a little thought, the pharmacist realized what the treasurer‚Äôs plan must be, and he concocted a counter plan, to make sure he survives and the treasurer dies. When the time came, the king summoned both of them. They drank the poisons as planned, the treasurer died, and the pharmacist survived. . What happened ? What was the treasurer‚Äôs plan ? What was the pharmacist counter-plan ? And did the king get what he wanted ? . . Hover to show the answer. . The treasurer‚Äôs plan was to drink a weak poison before going to the king‚Äôs challenge, and bring water to the challenge. In front of the king, he would drink the strong pharmacist‚Äôs poison, which would neutralize his, and then drink the water he brought. On the opposite, the pharmacist would drink water, followed by his own poison, and then die. . When the pharmacist realises that, he decides to also bring water. That way he drinks water both times, and the treasurer dies from his weak poison. .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/25/poison.html",
            "relUrl": "/riddle/2018/12/25/poison.html",
            "date": " ‚Ä¢ Dec 25, 2018"
        }
        
    
  
    
        ,"post17": {
            "title": "Lost father",
            "content": "Question: A mother is 21 years older than her child. In 6 years the mother will be 5 times older than her baby. . Where is the father? . Hover to show the answer. . M = mother | C = child . | M = C + 21 | M + 6 = 5* (C + 6) . | Solving gives C = -3/4 | -3/4 year means minus 9 months. If the child is -9 months, it means that the father is with the mother, in their bed‚Ä¶ | .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/25/lost-father.html",
            "relUrl": "/riddle/2018/12/25/lost-father.html",
            "date": " ‚Ä¢ Dec 25, 2018"
        }
        
    
  
    
        ,"post18": {
            "title": "Gold coins",
            "content": "There is a stack of 100 coins. Each coin has a silver side and a golden side. 20 coins are silver side up. The rest is golden side up. You are in a totally dark room. How do you make 2 stacks that contain as many coins with silver side up ? (we don‚Äôt care about how many are golden side up in each group) There is no way to differentiate the coins in the dark (by touch or other means.) . Hover to show the answer. . You take 20 coins from the stack and flip them to create another stack. The two stacks are the original stack withtout the 20 coins, and the stack of 20 coins you created. . If the 20 coins you select are golden side up, then you will flip them and have 20 silver coins in each stack. . If the 20 coins you select are silver side up, then you will flip them and have 0 silver coins in each stack. . If the 20 coins you select contain n silver and 20-n gold coins, then you will flip them and have 20-n silver coins in each stack. .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/25/gold-coins.html",
            "relUrl": "/riddle/2018/12/25/gold-coins.html",
            "date": " ‚Ä¢ Dec 25, 2018"
        }
        
    
  
    
        ,"post19": {
            "title": "Cats and mouses",
            "content": "If 3 cats take 3 minutes to catch 3 mouses, how many cats are needed to catch 100 mouses in 100 minutes ? . . Hover to show the answer. . You also need 3 cats. . If 3 cats take 3 minutes to catch 3 mouses, it means each cat takes 3 minutes to catch its own mouse. In 100 minutes, each of those cats will catch on average 33.333‚Ä¶ mouses. So 3 cats are enough to catch 100 mouses in 100 minutes. . Said differently, if 3 cats take 3 minutes to catch 3 mouses, these 3 cats catch 1 mouse/minute. At that rate, they will capture 100 mouses in 100 minutes. .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/25/cats-and-mouses.html",
            "relUrl": "/riddle/2018/12/25/cats-and-mouses.html",
            "date": " ‚Ä¢ Dec 25, 2018"
        }
        
    
  
    
        ,"post20": {
            "title": "Broken plane",
            "content": "Imagine a 747 is sitting on a conveyor belt, as wide and long as a runway. The conveyor belt is designed to [try to ?] exactly match the speed of the wheels, moving in the opposite direction. Can the plane take off? (different answers if you consider the words between brackets or not) . Hover to show the answer. . Yes. (Please don‚Äôt hit me). . If by speed of the wheels, we consider the speed of the the center of the wheels relative to the ground, then the answer is yes. The plane doesn‚Äôt use its wheel to take off. The motor thrust don‚Äôt care that the wheels are spinning or sliding or doing nothing. You have to realize that this is not like a car, the force is no pushing at the wheels, the force making the plane go forward is pushing where the motors are attached. So the wheels are just going to spin faster, and if the conveyor belt is long enough, the plane will take off. . Another way to see it is that the wheels are just there to keep the plane above the ground. They can spin at the speed they want, if the plane can move forward, it will take off. . Now, if by speed of the wheels, we consider the angular speed of the wheels then it gets complicated. This would basically be like saying ‚Äúthe conveyor belt does everything possible to make the plane not move‚Äù. . At that point, if the plane moves forward, the problem becomes mathematically impossible. When the plane is not moving, the angular speed is 0. When the plane moves, the angular speed is, let‚Äôs say, 10 ms/s. Now if the conveyor belt matches that speed, it will spin at 10m/s. But since the plane is still moving forward, this will make the wheels go at 20m/s (We consider here that the friction of the wheels is 0, they don‚Äôt affect the plane). So now the conveyor belt will have to match that speed. If the conveyor belt just tries to match the angular speed of the wheels, it will just go faster and faster up to infinity. If the friction of the wheels is 0, you could still argue in that situation that the plane will take off, with the wheel spinning infinitely fast in the opposite direction. . If the friction of the wheels is bigger than 0, you could argue that the threadmill manages to stop the plane. . If instead of the treadmill ‚Äútrying to match‚Äù you have the treadmill ‚Äúexactly matching‚Äù the speed of the wheels, than this becomes impossible. As soon as the plane starts going forward, this becomes an impossible situation because the treadmill has to match a speed that grows as it would try to match it. . Note that it doesn‚Äôt mean that the plane does not take off, but that the problem is mathematically impossible. . You can have a look at the excellent explanation from Randall Monroe on this problem here .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/25/broken-plane.html",
            "relUrl": "/riddle/2018/12/25/broken-plane.html",
            "date": " ‚Ä¢ Dec 25, 2018"
        }
        
    
  
    
        ,"post21": {
            "title": "Lost in the woods",
            "content": "You are in a square forest of side 100km. You are 2km away from its border, but you don‚Äôt know in what direction or at what angle (It could be at 2km with an angle of 16.123 degree for example.) You need to exit the forest, but you can walk a maximum of 13km. What path do you follow ? . Hover to show the answer. . Click the link to view an example of a path shorter than 13 km. .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/18/lost-in-the-woods.html",
            "relUrl": "/riddle/2018/12/18/lost-in-the-woods.html",
            "date": " ‚Ä¢ Dec 18, 2018"
        }
        
    
  
    
        ,"post22": {
            "title": "Evil professor",
            "content": "The evil professor says to the students: you are going to have an exam next week. I‚Äôm not telling you which day, but I am telling you that it will be unexpected (i.e., the day of the exam you won‚Äôt be sure whether the exam is that day or not). I‚Äôm assuming that the week starts on Monday and ends on Friday. . So this is what the students think: . Okay, it can‚Äôt be on Friday, because if Friday comes and we haven‚Äôt had the test yet, then in the morning we‚Äôll surely know it‚Äôs that day. So it will be a day from Monday to Thursday. After pondering a bit, they realise that it can‚Äôt be Thursday either, for the same reason: If Thursday comes and they haven‚Äôt had the exam they‚Äôll think ‚Äúwe know it‚Äôs not Friday, therefore it has to be today‚Äù. So Thursday wouldn‚Äôt be a surprise either. Similarly (by induction), all days are eliminated ! . Then they are happy and think the evil professor made a promise he couldn‚Äôt hold, and don‚Äôt prepare for the test. However, the next week, on Wednesday, they have the exam. And all are surprised. . Where is the flaw in the logic ? . Hover to show the answer. . There is no answer to this‚Ä¶ I will try to explain later. .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/18/evil-professor.html",
            "relUrl": "/riddle/2018/12/18/evil-professor.html",
            "date": " ‚Ä¢ Dec 18, 2018"
        }
        
    
  
    
        ,"post23": {
            "title": "Lighter",
            "content": "You have a lighter and two lengths of rope that burn for exactly 1 hour each. They do not burn consistently so you cannot, for example, cut one in half and burn that. How can you use these two ropes to measure exactly 45 minutes ? . Hover to show the answer. . Light one rope from both ends and one rope from one end. . After 30 minutes, the first rope will be finished burning. . You have 30 minutes left of burning on the second rope. Light the second end of that rope so that it will burn for 15 minutes instead of 30 minutes. . The total burning time will be 45 minutes. .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/10/rope.html",
            "relUrl": "/riddle/2018/12/10/rope.html",
            "date": " ‚Ä¢ Dec 10, 2018"
        }
        
    
  
    
        ,"post24": {
            "title": "Prisoners and trees",
            "content": "Here is a logic puzzle that depends on the game theory concept of common knowledge. . Can you figure it out? Alice and Bob are taken prisoners by an evil logician. They are given one chance to be set free. Alice and Bob are placed in cells that have a view of a courtyard with trees. There are 20 trees in all, of which Alice sees 12 and Bob sees 8. Neither prisoner knows how many trees the other sees. But each prisoner is told the trees are partitioned between them: together they see all the trees, but individually no tree is seen by both of them. . . They have to figure out the total number of trees, but they are not allowed to communicate with each other. Each day the logician visits Alice in her cell and asks, ‚ÄúAre there 18 or 20 trees in total?‚Äù Alice has two choices: she can guess or pass. If Alice passes, then the logician visits Bob in his cell and asks the same question. Bob also can guess or pass. If Bob passes, then the logician retires for the night asks and repeats asking the questions the next day. Both prisoners know the procedure of how the logician is asking questions. There are consequences to guessing. If either person guesses incorrectly, then they are both trapped forever. If either person guesses correctly, however, then they are both set free immediately. Obviously they could guess and have a 50% chance. But can they do better? Is there a way they can escape with certainty? . Hover to show the answer. . This is a complex one. Answer is coming. .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/10/prisoners-trees.html",
            "relUrl": "/riddle/2018/12/10/prisoners-trees.html",
            "date": " ‚Ä¢ Dec 10, 2018"
        }
        
    
  
    
        ,"post25": {
            "title": "Peeking",
            "content": "Jack is looking at Anne, but Anne is looking at George. Jack is married, but George is not. Is a married person looking at an unmarried person? . A: Yes | B: No | C: Cannot be determined | . Hover to show the answer. . Yes. . There are only two possible cases: either Anne is married, or she is not. . If she is married, then a married person (Jack) is looking at an unmarried person (Anne). If she is not married, then a married person (Anne) is looking at an unmarried person (George). .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/10/peeking.html",
            "relUrl": "/riddle/2018/12/10/peeking.html",
            "date": " ‚Ä¢ Dec 10, 2018"
        }
        
    
  
    
        ,"post26": {
            "title": "Exception",
            "content": "Find the exception . . Hover to show the answer. . B is the exception. . It is the only one that is only one difference away from every other one. . A and B have radius in difference | A and C have color in difference | A and D have snowfloake in difference | A and E have shape in diffrence | A and F have border in difference. | . If you select another one, like D, it can have 2 differences to some. .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/10/one-out.html",
            "relUrl": "/riddle/2018/12/10/one-out.html",
            "date": " ‚Ä¢ Dec 10, 2018"
        }
        
    
  
    
        ,"post27": {
            "title": "Magic logic",
            "content": "At a party last night, my friends Alice and Bob did a magic trick. Any ideas how it worked? . Alice shuffled a pack of cards, and asked me to take five. I looked at them. She put the rest of the pack down on the table. Alice asked for my cards. She had a look at them. She gave four of them to Bob (he was across the table), and the fifth back to me. Bob looked at the four cards for a while. Then Bob looked at me, and named the card I was holding. He was right. I‚Äôm quite sure he couldn‚Äôt have seen it (we weren‚Äôt sitting by a mirror). . They did the trick again later to someone else. I watched for funny business. Alice didn‚Äôt say anything to Bob, so I don‚Äôt think they have a code. Also Alice is famously clumsy, so I doubt it was sleight of hand. . . Hover to show the answer. . When Alice had the five cards in hands, she needs to find a way to select one, and communicate which one she selected to Bob by showing Bob the four other cards. . Here is the way to do that: . In the five cards, there are always two of the same suite. | Alice is going to chose one of those two and tell which one to Bob using the 3 remaining cards. | The thing to notice is: when having two cards of the same suite, they are alway 6 cards appart in some direction. For example 7-heart and 9-heart are 2 apart. But queen-spade and 2-spade are 3 apart: queen-king-ace-2. | Now since there are 12 different cards per suite, two cards are always going to be maximum 6 steps away from each other. | So the way to do it is: from the two cards of the same suite, pick the one that allows reaching the other by adding up to 6 steps. So with 7-heart and 9-heart, pick 7-heart. With 2-spade and queen-spade, pick queen-spade. | Let‚Äôs continue in the case where Alice picked the queen-spade. | Alice gives back to Bob 3 cards + the queen-spade on top. And Bob has to guess 2-spade. | The goal is now to make the number ‚Äò3‚Äô with the 3 remaining random cards. That way Bob knows he has to add ‚Äò3‚Äô to the queen-spade, which will give him the 2-spade. | This is easily doable. With 3 cards you can make 6 combinations. Just assign each of them to a number. | Here is a way to do this: The 3 cards can always be ordered by value. So you can have card-1, card-2 and card-3. The six orders could be: [1-2-3, 1-3-2, 2-1-3, 2-3-1, 3-1-2, 3-2-1]. Here we would select the order ‚Äò2-1-3‚Äô to represent the number 3. | So Bob sees the queen-spade, and then 3 cards that represent the number ‚Äò3‚Äô. He can now guess the 2-spade. | .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/10/magic-loop.html",
            "relUrl": "/riddle/2018/12/10/magic-loop.html",
            "date": " ‚Ä¢ Dec 10, 2018"
        }
        
    
  
    
        ,"post28": {
            "title": "Gender",
            "content": "A lady has two children. One is a boy. What are the chances of the other child also being a boy? . . Hover to show the answer. . This one is more complicated that I previously thought‚Ä¶ .",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/10/gender.html",
            "relUrl": "/riddle/2018/12/10/gender.html",
            "date": " ‚Ä¢ Dec 10, 2018"
        }
        
    
  
    
        ,"post29": {
            "title": "Blue eyes",
            "content": "A group of people with assorted eye colors live on an island. They are all perfect logicians ‚Äì if a conclusion can be logically deduced, they will do it instantly. No one knows the color of their eyes. Every night at midnight, a ferry stops at the island. Any islanders who have figured out the color of their own eyes then leave the island, and the rest stay. Everyone can see everyone else at all times and keeps a count of the number of people they see with each eye color (excluding themselves), but they cannot otherwise communicate. Everyone on the island knows all the rules in this paragraph. . On this island there are 100 blue-eyed people, 100 brown-eyed people, and the Guru (she happens to have green eyes). So any given blue-eyed person can see 100 people with brown eyes and 99 people with blue eyes (and one with green), but that does not tell him his own eye color; as far as he knows the totals could be 101 brown and 99 blue. Or 100 brown, 99 blue, and he could have red eyes. . The Guru is allowed to speak once (let‚Äôs say at noon), on one day in all their endless years on the island. Standing before the islanders, she says the following: . ‚ÄúI can see someone who has blue eyes.‚Äù . Who leaves the island, and on what night? . There are no mirrors or reflecting surfaces, nothing dumb. It is not a trick question, and the answer is logical. It doesn‚Äôt depend on tricky wording or anyone lying or guessing, and it doesn‚Äôt involve people doing something silly like creating a sign language or doing genetics. The Guru is not making eye contact with anyone in particular; she‚Äôs simply saying ‚ÄúI count at least one blue-eyed person on this island who isn‚Äôt me.‚Äù . And lastly, the answer is not ‚Äúno one leaves.‚Äù . Hover to show the answer. . All the 100 blue-eye people leave the island on the 100th night. . The following chart explains why using 4 people. The reflexion is the same, simply longer, for 100 persons. Click on the chart to open it in a new window. . The information &quot;There is at least one blue-eyed person&quot; is needed !The information &quot;There is at least one blue-eyed person&quot; is needed !If I am blue-eyed, he looks at 3 blue-eyed personsIf I am blue-eyed, he looks at 3 blue-eyed personsIf I am brown eyed, he looks at only 2 other blue-eyedIf I am brown eyed, he looks at only 2 other blue-eyedWhen I look at 3 blue-eyed person, I wonder if I am myself blue eyed.When I look at 3 blue-eyed person, I wonder if I am myself blue eyed.I consider myself blue-eyedI consider myself blue-eyedI consider myself brown-eyedI consider myself brown-eyedMEMEI wonder if this guy thinks he is blue eyed or not (I wonder the same for the 2 other, the reflexion is the same)[Not supported by viewer]MEMEAABBCCHe considers himself blue-eyedHe considers himself blue-eyedHe considers himself brown-eyedHe considers himself brown-eyedMEMEAABBCCIf A considers himself blue-eyed, there are 3 blue-eyed persons.If A considers himself blue-eyed, there are 3 blue-eyed persons.MEMEA[Not supported by viewer]BBCCIf A doesn&#39;t consider himself blue-eyed, he will try to understand what the 2 last one are going to do¬†[Not supported by viewer]This last guy could think he is brown-eyedThis last guy could think he is brown-eyedThis last guy could think he is blue-eyedThis last guy could think he is blue-eyedMEMEAABBCCThen there are 2 blue-eyed persons¬†Then there are 2 blue-eyed persons¬†MEMEAABBCCThen he looks at a blue-eye guy that is aloneThen he looks at a blue-eye guy that is aloneMEMEAABBCCIf someone comes and say &quot;There is at least one blue-eyed person&quot;, than person B will see 3 brown-eyed and leave ![Not supported by viewer]If someone comes and say &quot;There is at least one blue-eyed person&quot;, Than, no one will dare to leave the first day ![Not supported by viewer]After one day, we know which one of those 2 supposed situations is the true one.Either B left, and there was only 1 blue-eyed person.Or B stays, and C will understand that he is blue after one day.¬†[Not supported by viewer]However, if after two days, B and C are not leaving...A will understand that his supposition that he is brown might be false.The third day, A, B and C are going to leave[Not supported by viewer]If the 3rd day no one leaves..¬†I should understand that the supposition that I am brown can not hold.¬†Therefore, we are all blue-eyed.The 4th day, we all leave.[Not supported by viewer]The problem is the same with 4 persons and with 100 persons.This is the concept of common-knowledge.¬†At first, I might think that it is common knowledge that there are at least 3 blue-eyed person. And I might think that the sentence &quot;there is at least one blue-eyed person&quot; is useless, since everyone sees at least two blue eyed person.¬†But we don&#39;t all have the same knowledge. If I am browm eyed, other people might only see 2 blue-eyed persons. Those persons will think that other could only see 1 blue-eyed person. And those persons that see only one blue-eyed person could think that someone sees no blue-eyed persons.¬†This means that, until someone says &quot;I see at least one blue eyed person&quot; and 4 day passes, we don&#39;t all have the same knowledge about the world.The time it takes to reach common knowledge is the time the persons have to wait before leaving.¬†[Not supported by viewer]Yes this is comic sans ms :)Yes this is comic sans ms :)&lt;br&gt;",
            "url": "https://rcambier.github.io/blog/riddle/2018/12/10/blue-eyes.html",
            "relUrl": "/riddle/2018/12/10/blue-eyes.html",
            "date": " ‚Ä¢ Dec 10, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Ai from scratch",
          "content": "When I learn about a new ML algorithm, the best way for me to understand it is to look at a very basic version written from scratch in Python. . This is a compilation of notebooks with very simple implementations of some ML algorithms. I try to only use numpy and scipy. . Linear regression &amp; Logistic regression Ai from scratch",
          "url": "https://rcambier.github.io/blog/ai-from-scratch/",
          "relUrl": "/ai-from-scratch/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Riddles",
          "content": "Riddles ! . I haven&#39;t invented any of those. I just found them online and thought they were good ones. . The riddles are ranked by some measure of difficulty. . Cats and mouses (difficulty 1) If 3 cats take 3 minutes to catch 3 mouses, ... &lt;/div&gt; Peeking (difficulty 1) Jack is looking at Anne, but Anne is looking... &lt;/div&gt; Potatoes (difficulty 1.5) A farmer has 100kg of potatoes. At the start... &lt;/div&gt; Simple Maths (difficulty 1.5) How do you make 26 by using 5, 5, 5 and 1. Y... &lt;/div&gt; Surprise birthday (difficulty 2) Albert and Bernard just become friends with ... &lt;/div&gt; Reversed number (difficulty 2) Can you find a 4 digits number that gets rev... &lt;/div&gt; The rope triangle (difficulty 2) You cut a rope in a random place. Then again... &lt;/div&gt; The blind pill (difficulty 2) You are blind. You have 2 blue pills and 2 ... &lt;/div&gt; Exception (difficulty 2) Find the exception . Gold coins (difficulty 2) There is a stack of 100 coins. Each coin has... &lt;/div&gt; Lighter (difficulty 2) You have a lighter and two lengths of rope t... &lt;/div&gt; Running in circle (difficulty 2) If I can make it around a running track once... &lt;/div&gt; Lost father (difficulty 2.5) Question: A mother is 21 years older than he... &lt;/div&gt; King&#39;s poison (difficulty 2.5) In a far away land, it was known that if you... &lt;/div&gt; 4 Pieces (difficulty 2.5) There are 4 coins on a quadrant. They are no... &lt;/div&gt; Prisoner&#39;s switch (difficulty 3) The warden meets with 23 new prisoners when ... &lt;/div&gt; Magic logic (difficulty 3) At a party last night, my friends Alice and ... &lt;/div&gt; Lost in the woods (difficulty 3.5) You are in a square forest of side 100km. Yo... &lt;/div&gt; Prisoners and trees (difficulty 4) Here is a logic puzzle that depends on the g... &lt;/div&gt; Blue eyes (difficulty 5) A group of people with assorted eye colors l... &lt;/div&gt; Broken plane (difficulty 10) Imagine a 747 is sitting on a conveyor belt,... &lt;/div&gt; The newcomb poison (difficulty 99) You are put into a room with a suitcase and ... &lt;/div&gt; Evil professor (difficulty 99) The evil professor says to the students: you... &lt;/div&gt; Gender (difficulty 99) A lady has two children. One is a boy. What ... &lt;/div&gt; Three‚Äòs a Crowd (difficulty 99) From: http://www.twinbear.com/riddles.html . &lt;/div&gt; . . . . .",
          "url": "https://rcambier.github.io/blog/riddles/",
          "relUrl": "/riddles/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  
  

  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://rcambier.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}